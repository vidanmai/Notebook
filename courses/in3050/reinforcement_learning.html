<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>Reinforcement Learning</title><style type='text/css'>html {overflow-x: initial !important;}:root { --bg-color:#ffffff; --text-color:#333333; --select-text-bg-color:#B5D6FC; --select-text-font-color:auto; --monospace:"Lucida Console",Consolas,"Courier",monospace; }
html { font-size: 14px; background-color: var(--bg-color); color: var(--text-color); font-family: "Helvetica Neue", Helvetica, Arial, sans-serif; -webkit-font-smoothing: antialiased; }
body { margin: 0px; padding: 0px; height: auto; bottom: 0px; top: 0px; left: 0px; right: 0px; font-size: 1rem; line-height: 1.42857; overflow-x: hidden; background: inherit; tab-size: 4; }
iframe { margin: auto; }
a.url { word-break: break-all; }
a:active, a:hover { outline: 0px; }
.in-text-selection, ::selection { text-shadow: none; background: var(--select-text-bg-color); color: var(--select-text-font-color); }
#write { margin: 0px auto; height: auto; width: inherit; word-break: normal; overflow-wrap: break-word; position: relative; white-space: normal; overflow-x: visible; padding-top: 40px; }
#write.first-line-indent p { text-indent: 2em; }
#write.first-line-indent li p, #write.first-line-indent p * { text-indent: 0px; }
#write.first-line-indent li { margin-left: 2em; }
.for-image #write { padding-left: 8px; padding-right: 8px; }
body.typora-export { padding-left: 30px; padding-right: 30px; }
.typora-export .footnote-line, .typora-export li, .typora-export p { white-space: pre-wrap; }
@media screen and (max-width: 500px) {
  body.typora-export { padding-left: 0px; padding-right: 0px; }
  #write { padding-left: 20px; padding-right: 20px; }
  .CodeMirror-sizer { margin-left: 0px !important; }
  .CodeMirror-gutters { display: none !important; }
}
#write li > figure:last-child { margin-bottom: 0.5rem; }
#write ol, #write ul { position: relative; }
img { max-width: 100%; vertical-align: middle; }
button, input, select, textarea { color: inherit; font: inherit; }
input[type="checkbox"], input[type="radio"] { line-height: normal; padding: 0px; }
*, ::after, ::before { box-sizing: border-box; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p, #write pre { width: inherit; }
#write h1, #write h2, #write h3, #write h4, #write h5, #write h6, #write p { position: relative; }
p { line-height: inherit; }
h1, h2, h3, h4, h5, h6 { break-after: avoid-page; break-inside: avoid; orphans: 2; }
p { orphans: 4; }
h1 { font-size: 2rem; }
h2 { font-size: 1.8rem; }
h3 { font-size: 1.6rem; }
h4 { font-size: 1.4rem; }
h5 { font-size: 1.2rem; }
h6 { font-size: 1rem; }
.md-math-block, .md-rawblock, h1, h2, h3, h4, h5, h6, p { margin-top: 1rem; margin-bottom: 1rem; }
.hidden { display: none; }
.md-blockmeta { color: rgb(204, 204, 204); font-weight: 700; font-style: italic; }
a { cursor: pointer; }
sup.md-footnote { padding: 2px 4px; background-color: rgba(238, 238, 238, 0.7); color: rgb(85, 85, 85); border-radius: 4px; cursor: pointer; }
sup.md-footnote a, sup.md-footnote a:hover { color: inherit; text-transform: inherit; text-decoration: inherit; }
#write input[type="checkbox"] { cursor: pointer; width: inherit; height: inherit; }
figure { overflow-x: auto; margin: 1.2em 0px; max-width: calc(100% + 16px); padding: 0px; }
figure > table { margin: 0px !important; }
tr { break-inside: avoid; break-after: auto; }
thead { display: table-header-group; }
table { border-collapse: collapse; border-spacing: 0px; width: 100%; overflow: auto; break-inside: auto; text-align: left; }
table.md-table td { min-width: 32px; }
.CodeMirror-gutters { border-right: 0px; background-color: inherit; }
.CodeMirror-linenumber { user-select: none; }
.CodeMirror { text-align: left; }
.CodeMirror-placeholder { opacity: 0.3; }
.CodeMirror pre { padding: 0px 4px; }
.CodeMirror-lines { padding: 0px; }
div.hr:focus { cursor: none; }
#write pre { white-space: pre-wrap; }
#write.fences-no-line-wrapping pre { white-space: pre; }
#write pre.ty-contain-cm { white-space: normal; }
.CodeMirror-gutters { margin-right: 4px; }
.md-fences { font-size: 0.9rem; display: block; break-inside: avoid; text-align: left; overflow: visible; white-space: pre; background: inherit; position: relative !important; }
.md-diagram-panel { width: 100%; margin-top: 10px; text-align: center; padding-top: 0px; padding-bottom: 8px; overflow-x: auto; }
#write .md-fences.mock-cm { white-space: pre-wrap; }
.md-fences.md-fences-with-lineno { padding-left: 0px; }
#write.fences-no-line-wrapping .md-fences.mock-cm { white-space: pre; overflow-x: auto; }
.md-fences.mock-cm.md-fences-with-lineno { padding-left: 8px; }
.CodeMirror-line, twitterwidget { break-inside: avoid; }
.footnotes { opacity: 0.8; font-size: 0.9rem; margin-top: 1em; margin-bottom: 1em; }
.footnotes + .footnotes { margin-top: 0px; }
.md-reset { margin: 0px; padding: 0px; border: 0px; outline: 0px; vertical-align: top; background: 0px 0px; text-decoration: none; text-shadow: none; float: none; position: static; width: auto; height: auto; white-space: nowrap; cursor: inherit; -webkit-tap-highlight-color: transparent; line-height: normal; font-weight: 400; text-align: left; box-sizing: content-box; direction: ltr; }
li div { padding-top: 0px; }
blockquote { margin: 1rem 0px; }
li .mathjax-block, li p { margin: 0.5rem 0px; }
li { margin: 0px; position: relative; }
blockquote > :last-child { margin-bottom: 0px; }
blockquote > :first-child, li > :first-child { margin-top: 0px; }
.footnotes-area { color: rgb(136, 136, 136); margin-top: 0.714rem; padding-bottom: 0.143rem; white-space: normal; }
#write .footnote-line { white-space: pre-wrap; }
@media print {
  body, html { border: 1px solid transparent; height: 99%; break-after: avoid; break-before: avoid; }
  #write { margin-top: 0px; padding-top: 0px; border-color: transparent !important; }
  .typora-export * { -webkit-print-color-adjust: exact; }
  html.blink-to-pdf { font-size: 13px; }
  .typora-export #write { padding-left: 32px; padding-right: 32px; padding-bottom: 0px; break-after: avoid; }
  .typora-export #write::after { height: 0px; }
}
.footnote-line { margin-top: 0.714em; font-size: 0.7em; }
a img, img a { cursor: pointer; }
pre.md-meta-block { font-size: 0.8rem; min-height: 0.8rem; white-space: pre-wrap; background: rgb(204, 204, 204); display: block; overflow-x: hidden; }
p > .md-image:only-child:not(.md-img-error) img, p > img:only-child { display: block; margin: auto; }
p > .md-image:only-child { display: inline-block; width: 100%; }
#write .MathJax_Display { margin: 0.8em 0px 0px; }
.md-math-block { width: 100%; }
.md-math-block:not(:empty)::after { display: none; }
[contenteditable="true"]:active, [contenteditable="true"]:focus, [contenteditable="false"]:active, [contenteditable="false"]:focus { outline: 0px; box-shadow: none; }
.md-task-list-item { position: relative; list-style-type: none; }
.task-list-item.md-task-list-item { padding-left: 0px; }
.md-task-list-item > input { position: absolute; top: 0px; left: 0px; margin-left: -1.2em; margin-top: calc(1em - 10px); border: none; }
.math { font-size: 1rem; }
.md-toc { min-height: 3.58rem; position: relative; font-size: 0.9rem; border-radius: 10px; }
.md-toc-content { position: relative; margin-left: 0px; }
.md-toc-content::after, .md-toc::after { display: none; }
.md-toc-item { display: block; color: rgb(65, 131, 196); }
.md-toc-item a { text-decoration: none; }
.md-toc-inner:hover { text-decoration: underline; }
.md-toc-inner { display: inline-block; cursor: pointer; }
.md-toc-h1 .md-toc-inner { margin-left: 0px; font-weight: 700; }
.md-toc-h2 .md-toc-inner { margin-left: 2em; }
.md-toc-h3 .md-toc-inner { margin-left: 4em; }
.md-toc-h4 .md-toc-inner { margin-left: 6em; }
.md-toc-h5 .md-toc-inner { margin-left: 8em; }
.md-toc-h6 .md-toc-inner { margin-left: 10em; }
@media screen and (max-width: 48em) {
  .md-toc-h3 .md-toc-inner { margin-left: 3.5em; }
  .md-toc-h4 .md-toc-inner { margin-left: 5em; }
  .md-toc-h5 .md-toc-inner { margin-left: 6.5em; }
  .md-toc-h6 .md-toc-inner { margin-left: 8em; }
}
a.md-toc-inner { font-size: inherit; font-style: inherit; font-weight: inherit; line-height: inherit; }
.footnote-line a:not(.reversefootnote) { color: inherit; }
.md-attr { display: none; }
.md-fn-count::after { content: "."; }
code, pre, samp, tt { font-family: var(--monospace); }
kbd { margin: 0px 0.1em; padding: 0.1em 0.6em; font-size: 0.8em; color: rgb(36, 39, 41); background: rgb(255, 255, 255); border: 1px solid rgb(173, 179, 185); border-radius: 3px; box-shadow: rgba(12, 13, 14, 0.2) 0px 1px 0px, rgb(255, 255, 255) 0px 0px 0px 2px inset; white-space: nowrap; vertical-align: middle; }
.md-comment { color: rgb(162, 127, 3); opacity: 0.8; font-family: var(--monospace); }
code { text-align: left; vertical-align: initial; }
a.md-print-anchor { white-space: pre !important; border-width: initial !important; border-style: none !important; border-color: initial !important; display: inline-block !important; position: absolute !important; width: 1px !important; right: 0px !important; outline: 0px !important; background: 0px 0px !important; text-decoration: initial !important; text-shadow: initial !important; }
.md-inline-math .MathJax_SVG .noError { display: none !important; }
.html-for-mac .inline-math-svg .MathJax_SVG { vertical-align: 0.2px; }
.md-math-block .MathJax_SVG_Display { text-align: center; margin: 0px; position: relative; text-indent: 0px; max-width: none; max-height: none; min-height: 0px; min-width: 100%; width: auto; overflow-y: hidden; display: block !important; }
.MathJax_SVG_Display, .md-inline-math .MathJax_SVG_Display { width: auto; margin: inherit; display: inline-block !important; }
.MathJax_SVG .MJX-monospace { font-family: var(--monospace); }
.MathJax_SVG .MJX-sans-serif { font-family: sans-serif; }
.MathJax_SVG { display: inline; font-style: normal; font-weight: 400; line-height: normal; zoom: 90%; text-indent: 0px; text-align: left; text-transform: none; letter-spacing: normal; word-spacing: normal; overflow-wrap: normal; white-space: nowrap; float: none; direction: ltr; max-width: none; max-height: none; min-width: 0px; min-height: 0px; border: 0px; padding: 0px; margin: 0px; }
.MathJax_SVG * { transition: none 0s ease 0s; }
.MathJax_SVG_Display svg { vertical-align: middle !important; margin-bottom: 0px !important; margin-top: 0px !important; }
.os-windows.monocolor-emoji .md-emoji { font-family: "Segoe UI Symbol", sans-serif; }
.md-diagram-panel > svg { max-width: 100%; }
[lang="flow"] svg, [lang="mermaid"] svg { max-width: 100%; height: auto; }
[lang="mermaid"] .node text { font-size: 1rem; }
table tr th { border-bottom: 0px; }
video { max-width: 100%; display: block; margin: 0px auto; }
iframe { max-width: 100%; width: 100%; border: none; }
.highlight td, .highlight tr { border: 0px; }
svg[id^="mermaidChart"] { line-height: 1em; }
mark { background: rgb(255, 255, 0); color: rgb(0, 0, 0); }
.md-html-inline .md-plain, .md-html-inline strong, mark .md-inline-math, mark strong { color: inherit; }
mark .md-meta { color: rgb(0, 0, 0); opacity: 0.3 !important; }


:root {
  --side-bar-bg-color: #fff;
  --control-text-color: #777;
  --select-text-bg-color: #ffafa3;
  --active-file-text-color: #262626;
  --active-file-border-color: #f22f27;
  /* --active-file-bg-color: #fff3f0; */
  --primary-color: #f22f27;

  /* 中性色 */
  --mid-1: #ffffff;
  --mid-2: #fafafa;
  --mid-3: #f5f5f5;
  --mid-4: #f0f0f0;
  --mid-5: #d9d9d9;
  --mid-6: #bfbfbf;
  --mid-7: #8c8c8c;
  --mid-8: #595959;
  --mid-9: #434343;
  --mid-10: #262626;
  --mid-11: #1f1f1f;
  --mid-12: #141414;
  --mid-13: #000000;
  /* 主题色 */
  --main-1: #fff3f0;
  --main-2: #ffd4cc;
  --main-3: #ffafa3;
  --main-4: #ff887a;
  --main-5: #ff5d52;
  --main-6: #f22f27;
  --main-7: #cc1616;
  --main-8: #a60a0f;
  --main-9: #80010a;
  --main-10: #590009;
}

/* cyrillic-ext */
/* cyrillic */
/* greek-ext */
/* greek */
/* vietnamese */
/* latin-ext */
/* latin */
/* cyrillic-ext */
/* cyrillic */
/* greek-ext */
/* greek */
/* vietnamese */
/* latin-ext */
/* latin */
/* cyrillic-ext */
/* cyrillic */
/* greek-ext */
/* greek */
/* vietnamese */
/* latin-ext */
/* latin */
/* cyrillic-ext */
/* cyrillic */
/* greek-ext */
/* greek */
/* vietnamese */
/* latin-ext */
/* latin */
html {
  font-size: 14px;
}

body {
  font-family: Source Sans Pro, Helvetica Neue, Arial, sans-serif !important;
  color: var(--mid-10);
  -webkit-font-smoothing: antialiased;
  line-height: 1.8rem;
  letter-spacing: 0;
  margin: 0;
  overflow-x: hidden;
}

#write {
  max-width: 860px;
  margin: 0 auto;
  padding: 20px 30px 160px;
}

#write p {
  line-height: 1.8rem;
  word-spacing: 0.05rem;
}

#write ol li {
  margin-left: -4px;
}
#write ol li p {
  margin-left: 4px;
}


#write ul {
  line-height: 2rem;
}

#write > ul:first-child,
#write > ol:first-child {
  margin-top: 30px;
}

body > *:first-child {
  margin-top: 0 !important;
}

body > *:last-child {
  margin-bottom: 0 !important;
}

a {
  color: var(--main-6);
  font-weight: 500;
  padding: 0 2px;
  text-decoration: none;
}

/* 链接 */
#write a {
  border-bottom: 1px solid var(--main-6);
  color: var(--main-6);
  text-decoration: none;
}

/* 目录 */
#write a.md-toc-inner {
  line-height: 1.6;
  white-space: pre-line;
  border-bottom: none;
}

#write a:hover {
  border-bottom: 2px solid var(--main-6);
  color: var(--main-7);
}

h1,
h2,
h3,
h4,
h5,
h6 {
  position: relative;
  margin-top: 1rem;
  margin-bottom: 1rem;
  font-weight: bold;
  line-height: 1.4;
  cursor: text;
}

h1:hover a.anchor,
h2:hover a.anchor,
h3:hover a.anchor,
h4:hover a.anchor,
h5:hover a.anchor,
h6:hover a.anchor {
  text-decoration: none;
}

h1 tt,
h1 code,
h2 tt,
h2 code,
h3 tt,
h3 code,
h4 tt,
h4 code,
h5 tt,
h5 code,
h6 tt,
h6 code {
  font-size: inherit !important;
}

h2 a,
h3 a {
  color: var(--mid-9);
}

h1 {
  text-align: center;
  padding-bottom: 0.3em;
  font-size: 2em;
  line-height: 1.2;
  margin: 2.4em auto 1.2em;
}

h1:after {
  content: '';
  display: block;
  margin: 0.2em auto 0;
  width: 100px;
  height: 2px;
  border-bottom: 2px dashed var(--main-6);
}

h2 {
  margin: 2em auto 1.4em;
  padding-left: 6px;
  line-height: 1.4;
  font-size: 1.6em;
  border-left: 6px solid var(--main-6);
}

h3 {
  margin: 1.6em auto 1.2em;
  font-size: 1.35em;
  line-height: 1.43;
}

/*--- 三级标题左边的小圆点 ---*/
h3:before {
  content: '';
  border-radius: 50%;
  background-color: var(--main-5);
  display: inline-block;
  width: 6px;
  height: 6px;
  vertical-align: middle;
  margin-bottom: 0.18em;
  margin-right: 6px;
}

/* 三级四级标题点击后左边的提示图标 */
#write > h3.md-focus:before,
#write > h4.md-focus:before {
  width: auto;
  height: auto;
  background-color: var(--main-5);
  color: var(--mid-1);
}

h4 {
  margin-top: 1.4em;
  font-size: 1.2em;
}

/*--- 四级标题左边的小短线 ---*/
h4:before {
  background-color: var(--main-4);
  content: '';
  width: 6px;
  display: inline-block;
  height: 2px;
  vertical-align: middle;
  margin-bottom: 0.18em;
  margin-right: 8px;
}

h5 {
  font-size: 1em;
}

h6 {
  font-size: 1em;
  color: var(--mid-7);
}

p,
blockquote,
ul,
ol,
dl,
table {
  margin: 0.8em 0;
}

li > ol,
li > ul {
  margin: 0 0;
}

hr {
  width: 100%;
  height: 1px;
  padding: 0;
  margin: 46px auto 64px;
  background-color: var(--main-4);
  border: 0;
  overflow: hidden;
  box-sizing: content-box;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0;
}

body > h1:first-child {
  margin-top: 0;
  padding-top: 0;
}

body > h1:first-child + h2 {
  margin-top: 0;
  padding-top: 0;
}

body > h3:first-child,
body > h4:first-child,
body > h5:first-child,
body > h6:first-child {
  margin-top: 0;
  padding-top: 0;
}

a:first-child h1,
a:first-child h2,
a:first-child h3,
a:first-child h4,
a:first-child h5,
a:first-child h6 {
  margin-top: 0;
  padding-top: 0;
}

h1 p,
h2 p,
h3 p,
h4 p,
h5 p,
h6 p {
  margin-top: 0;
}

li p.first {
  display: inline-block;
}

ul,
ol {
  padding-left: 30px;
}

ul:first-child,
ol:first-child {
  margin-top: 0;
}

ul:last-child,
ol:last-child {
  margin-bottom: 0;
}

/*--- 引用块 ---*/
blockquote {
  position: relative;
  border: none;
  border-left: 2px solid var(--main-6);
  color: var(--mid-7);
  font-size: 1em;
  font-style: normal;
  padding: 24px 16px 12px;
  margin: 24px 0 36px;
  line-height: 1.6;
  text-indent: 0;
}

blockquote blockquote {
  padding-right: 0;
}

blockquote a {
  color: var(--main-4);
}

blockquote:before {
  content: '“';
  position: absolute;
  left: 12px;
  top: 0;
  color: var(--main-6);
  font-size: 2em;
  font-family: Arial, serif;
  line-height: 1em;
  font-weight: 700;
}

table {
  margin: 36px auto;
  padding: 0;
  word-break: initial;
}

table tr {
  border-top: 1px solid #dfe2e5;
  margin: 0;
  padding: 0;
}

table tr:nth-child(2n),
thead {
  background-color: var(--mid-3);
}

table tr th {
  font-weight: bold;
  border: 1px solid #dfe2e5;
  border-bottom: 0;
  text-align: left;
  margin: 0;
  padding: 6px 13px;
}

table tr td {
  border: 1px solid #dfe2e5;
  text-align: left;
  margin: 0;
  padding: 6px 13px;
}

table tr th:first-child,
table tr td:first-child {
  margin-top: 0;
}

table tr th:last-child,
table tr td:last-child {
  margin-bottom: 0;
}

#write table thead th {
  background-color: #f2f2f2;
  text-align: center;
}

#write strong {
  padding: 0 1px;
}

#write em {
  padding: 0 5px 0 2px;
}

#write .CodeMirror-gutters {
  border-right: none;
}

/* 代码块 */
#write .md-fences {
  -webkit-font-smoothing: initial;
  margin: 1.8rem 0 2rem !important;
  line-height: 1.55rem;
  font-family: Source Code Pro, Roboto Mono, Source Sans Pro, 'Microsoft YaHei', '微软雅黑' !important;
  font-size: 0.9rem;
  word-wrap: normal;
  color: var(--mid-10);
}

#write .CodeMirror-wrap .CodeMirror-code pre {
  padding-left: 12px;
  line-height: 1.55rem;
}

#write .CodeMirror-cursors .CodeMirror-cursor {
  border-left: 2px solid var(--mid-4);
}

#write code,
tt {
  margin: 0 2px;
  padding: 2px 4px;
  border-radius: 2px;
  font-family: Source Code Pro, Roboto Mono, Source Sans Pro, 'Microsoft YaHei', '微软雅黑' !important;
  font-size: 0.92rem;
  color: var(--main-5);
  background-color: var(--main-1);
}

#write .md-footnote {
  color: var(--main-5);
  background-color: var(--main-1);
}

/* 流程图块 */
#write .md-diagram-panel {
  position: relative;
  margin: 24px auto;
}

#write .md-focus .md-diagram-panel {
  border: 1px solid var(--main-4);
  border-radius: 4px;
}

/* heighlight. */
#write mark {
  background-color: var(--main-4);
  border-radius: 2px;
  padding: 2px 4px;
  margin: 0 2px;
  color: #222;
  border-radius: 4px;
  font-weight: 500;
}

#write del {
  padding: 1px 2px;
}

.cm-s-inner .cm-link,
.cm-s-inner.cm-link {
  color: #22a2c9;
}

.cm-s-inner .cm-string {
  color: #22a2c9;
}

/* 任务列表小方框 */
.md-task-list-item > input {
  margin-left: -1.3em;
  margin-top: 0.3rem;
  -webkit-appearance: none;
}

.md-task-list-item > input:before {
  content: '';
  display: inline-block;
  width: 0.875rem;
  height: 0.875rem;
  vertical-align: middle;
  text-align: center;
  font-size: 0.8rem;
  color: var(--mid-1);
  border-radius: 2px;
  border: 1px solid var(--main-4);
  margin-top: -0.4rem;
  transition: all 0.2s linear;
}

.md-task-list-item > input:checked:before,
.md-task-list-item > input[checked]:before {
  content: '\2714';
  font-size: 0.625rem;
  color: var(--mid-1);
  border: 1px solid var(--main-6);
  background-color: var(--main-6);
}

@media print {
  html {
    font-size: 13px;
  }

  table,
  pre {
    page-break-inside: avoid;
  }

  pre {
    word-wrap: break-word;
  }
}

/* .md-fences {
  background-color: #f8f8f8;
} */

#write pre.md-meta-block {
  /* padding: 1rem; */
  font-size: 85%;
  line-height: 1.45;
  background-color: #f7f7f7;
  border: 0;
  border-radius: 3px;
  color: #777777;
  margin-top: 0 !important;
}

.mathjax-block > .code-tooltip {
  bottom: 0.375rem;
}

#write > h3.md-focus:before {
  left: -1.5625rem;
  top: 0.375rem;
}

#write > h4.md-focus:before {
  left: -1.5625rem;
  top: 0.285714286rem;
}

#write > h5.md-focus:before {
  left: -1.5625rem;
  top: 0.285714286rem;
}

#write > h6.md-focus:before {
  left: -1.5625rem;
  top: 0.285714286rem;
}

/*--- 图片 ---*/
.md-image {
  margin: 24px auto;
  border-radius: 4px;
}

.md-image img {
  border-radius: 4px;
}

/* 当 “![shadow-随便写]()”写时，会有阴影 */
.md-image img[alt|='shadow'] {
  box-shadow: 0 4px 24px -6px #ddd;
}

.md-image > .md-meta {
  border-radius: 3px;
  font-family: Consolas, 'Liberation Mono', Courier, monospace;
  padding: 2px 0 0 4px;
  font-size: 0.9em;
  color: inherit;
}

/* emoji */
.md-emoji-span:before {
  content: attr(data-emoji);
  font-weight: 400;
  vertical-align: inherit;
}

.md-tag {
  color: inherit;
}

.md-toc {
  margin-top: 20px;
  padding-bottom: 20px;
}

.sidebar-tabs {
  border-bottom: none;
}

#typora-quick-open {
  border: 1px solid #ddd;
  background-color: #f8f8f8;
}

#typora-quick-open-item {
  background-color: #fafafa;
  border-color: #fefefe #e5e5e5 #e5e5e5 #eee;
  border-style: solid;
  border-width: 1px;
}

#md-notification:before {
  top: 10px;
}

/** focus mode */

.on-focus-mode blockquote {
  border-left-color: rgba(85, 85, 85, 0.12);
}

header,
.context-menu,
.megamenu-content,
footer {
  font-family: 'Segoe UI', 'Arial', sans-serif;
}

.file-node-content:hover .file-node-icon,
.file-node-content:hover .file-node-open-state {
  visibility: visible;
}

.mac-seamless-mode #typora-sidebar {
  background-color: var(--side-bar-bg-color);
}

.md-lang {
  color: #b4654d;
}

.html-for-mac .context-menu {
  --item-hover-bg-color: #e6f0fe;
}

/* 侧边栏 */
.file-list-item.active {
  background: var(--active-file-bg-color);
  color: var(--active-file-text-color);
  border-left: 4px solid var(--main-6);
}

.file-tree-node.active > .file-node-background {
  background-color: var(--active-file-bg-color);
  border-left: 4px solid var(--main-6);
  border-color: var(--active-file-border-color);
}

/* ----------------- 代码块主题 ----------------- */

.cm-s-inner.CodeMirror {
  padding: 0.8rem 0 1rem;
  background-color: #292d3e;
  color: #a6accd;
  border-radius: 4px;
}

.cm-s-inner .CodeMirror-gutters {
  background: #292d3e;
  color: #676e95;
  border: none;
}

.cm-s-inner .CodeMirror-guttermarker,
.cm-s-inner .CodeMirror-guttermarker-subtle,
.cm-s-inner .CodeMirror-linenumber {
  color: #676e95;
}

.cm-s-inner .CodeMirror-cursor {
  border-left: 1px solid #ffcc00;
}

.cm-s-inner div.CodeMirror-selected {
  background: rgba(113, 124, 180, 0.2);
}

.cm-s-inner.CodeMirror-focused div.CodeMirror-selected {
  background: rgba(113, 124, 180, 0.2);
}

.cm-s-inner .CodeMirror-line::selection,
.cm-s-inner .CodeMirror-line > span::selection,
.cm-s-inner .CodeMirror-line > span > span::selection {
  background: rgba(128, 203, 196, 0.2);
}

.cm-s-inner .CodeMirror-line::-moz-selection,
.cm-s-inner .CodeMirror-line > span::-moz-selection,
.cm-s-inner .CodeMirror-line > span > span::-moz-selection {
  background: rgba(128, 203, 196, 0.2);
}

/* .cm-s-inner .CodeMirror-activeline-background {
    background: rgba(255, 255, 255, 0.5);
  } */

.cm-s-inner .cm-keyword {
  color: #c792ea;
}

.cm-s-inner .cm-operator {
  color: #89ddff;
}

.cm-s-inner .cm-variable-2 {
  color: #eeffff;
}

.cm-s-inner .cm-variable-3,
.cm-s-inner .cm-type {
  color: #f07178;
}

.cm-s-inner .cm-builtin {
  color: #ffcb6b;
}

.cm-s-inner .cm-atom {
  color: #f78c6c;
}

.cm-s-inner .cm-number {
  color: #ff5370;
}

.cm-s-inner .cm-def {
  color: #82aaff;
}

.cm-s-inner .cm-string {
  color: #c3e88d;
}

.cm-s-inner .cm-string-2 {
  color: #f07178;
}

.cm-s-inner .cm-comment {
  color: #676e95;
}

.cm-s-inner .cm-variable {
  color: #f07178;
}

.cm-s-inner .cm-tag {
  color: #ff5370;
}

.cm-s-inner .cm-meta {
  color: #ffcb6b;
}

.cm-s-inner .cm-attribute {
  color: #c792ea;
}

.cm-s-inner .cm-property {
  color: #c792ea;
}

.cm-s-inner .cm-qualifier {
  color: #decb6b;
}

.cm-s-inner .cm-variable-3,
.cm-s-inner .cm-type {
  color: #decb6b;
}

.cm-s-inner .cm-error {
  color: rgba(255, 255, 255, 1);
  background-color: #ff5370;
}

.cm-s-inner .CodeMirror-matchingbracket {
  text-decoration: underline;
  color: white !important;
}



</style>
</head>
<body class='typora-export os-windows' >
<div  id='write'  class = 'is-node'><h1><a name="reinforcement-learning" class="md-header-anchor"></a><span>Reinforcement Learning</span></h1><h3><a name="terminology" class="md-header-anchor"></a><span>Terminology</span></h3><ol start='' ><li><span>What is an agent?</span>
<span>An entity interacting within an environment according to a policy</span></li><li><span>What is an environment?</span>
<span>The entity with which the agent interact, afrom which the agent receives feedbacks and reward</span></li><li><span>What is a state?</span>
<span>A descriptor of the environment and the agent (a sufficient statistics of the history for the agent’s decision)</span></li><li><span>What is an action?</span>
<span>A way for the agent to interact with the environment</span></li><li><span>What is a policy?</span>
<span>A rule to choose actions given states</span></li><li><span>What is a reward?</span>
<span>A signal denoting how good the current action, and implicitly, the preceding ones were</span></li></ol><p>&nbsp;</p><h4><a name="applications-of-rl" class="md-header-anchor"></a><span>Applications of RL:</span></h4><p><img src="C:\Users\Vi Dan Ba Mai\AppData\Roaming\Typora\typora-user-images\image-20200513203029453.png" referrerpolicy="no-referrer" alt="image-20200513203029453"></p><h3><a name="types-of-machine-learning" class="md-header-anchor"></a><span>Types of machine learning</span></h3><p><img src="C:\Users\Vi Dan Ba Mai\AppData\Roaming\Typora\typora-user-images\image-20200514004329884.png" referrerpolicy="no-referrer" alt="image-20200514004329884"></p><p>&nbsp;</p><h4><a name="supervised-learning" class="md-header-anchor"></a><span>Supervised Learning</span></h4><p><span>We send in inputs through the machine learning model (Supervised Learning System) and we get output.</span></p><p><span>The training fixes and adjusts the system and its weights according to the desired (target) outputs, which is the </span><strong><em><span>training info</span></em></strong><span>.</span></p><p><img src="C:\Users\Vi Dan Ba Mai\AppData\Roaming\Typora\typora-user-images\image-20200514004537366.png" referrerpolicy="no-referrer" alt="image-20200514004537366"></p><h4><a name="reinforcement-learning-n26" class="md-header-anchor"></a><span>Reinforcement Learning</span></h4><p><span>The machine learning model is based on evaluation by “rewards” and “penalties”.</span></p><p><strong><span>Objective:</span></strong><span> Get as much reward as possible </span></p><p><img src="C:\Users\Vi Dan Ba Mai\AppData\Roaming\Typora\typora-user-images\image-20200514004553146.png" referrerpolicy="no-referrer" alt="image-20200514004553146"></p><h3><a name="the-reinforcement-learning-problem" class="md-header-anchor"></a><span>The reinforcement learning problem</span></h3><p><span>State, Action and Reward</span></p><h4><a name="learning-is-guided-by-the-reward" class="md-header-anchor"></a><span>Learning is guided by the reward</span></h4><ul><li><p><span>An infrequent numerical feedback indicating how well we are doing</span></p></li><li><p><span>Problems:</span></p><ul><li><span>The reward doesn’t tell us what we should have done!</span></li><li><span>The reward may be delayed - doesn’t always indicate when we made a mistake</span></li></ul></li></ul><h5><a name="the-reward-function" class="md-header-anchor"></a><span>The </span><em><span>reward</span></em><span> function</span></h5><ul><li><span>Corresponds to the fitness function of an evolutionary algorithm</span></li><li><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.972ex" height="1.936ex" viewBox="0 -535.3 1709.9 833.5" role="img" focusable="false" style="vertical-align: -0.693ex;"><defs><path stroke-width="0" id="E12-MJMATHI-72" d="M21 287Q22 290 23 295T28 317T38 348T53 381T73 411T99 433T132 442Q161 442 183 430T214 408T225 388Q227 382 228 382T236 389Q284 441 347 441H350Q398 441 422 400Q430 381 430 363Q430 333 417 315T391 292T366 288Q346 288 334 299T322 328Q322 376 378 392Q356 405 342 405Q286 405 239 331Q229 315 224 298T190 165Q156 25 151 16Q138 -11 108 -11Q95 -11 87 -5T76 7T74 17Q74 30 114 189T154 366Q154 405 128 405Q107 405 92 377T68 316T57 280Q55 278 41 278H27Q21 284 21 287Z"></path><path stroke-width="0" id="E12-MJMATHI-74" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path><path stroke-width="0" id="E12-MJMAIN-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path><path stroke-width="0" id="E12-MJMAIN-31" d="M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E12-MJMATHI-72" x="0" y="0"></use><g transform="translate(451,-150)"><use transform="scale(0.707)" xlink:href="#E12-MJMATHI-74" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E12-MJMAIN-2B" x="361" y="0"></use><use transform="scale(0.707)" xlink:href="#E12-MJMAIN-31" x="1139" y="0"></use></g></g></svg></span><script type="math/tex">r_{t+1}</script><span> is a function of </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="6.808ex" height="2.762ex" viewBox="0 -831.5 2931.2 1189" role="img" focusable="false" style="vertical-align: -0.83ex;"><defs><path stroke-width="0" id="E13-MJMAIN-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path stroke-width="0" id="E13-MJMATHI-73" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path stroke-width="0" id="E13-MJMATHI-74" d="M26 385Q19 392 19 395Q19 399 22 411T27 425Q29 430 36 430T87 431H140L159 511Q162 522 166 540T173 566T179 586T187 603T197 615T211 624T229 626Q247 625 254 615T261 596Q261 589 252 549T232 470L222 433Q222 431 272 431H323Q330 424 330 420Q330 398 317 385H210L174 240Q135 80 135 68Q135 26 162 26Q197 26 230 60T283 144Q285 150 288 151T303 153H307Q322 153 322 145Q322 142 319 133Q314 117 301 95T267 48T216 6T155 -11Q125 -11 98 4T59 56Q57 64 57 83V101L92 241Q127 382 128 383Q128 385 77 385H26Z"></path><path stroke-width="0" id="E13-MJMAIN-2C" d="M78 35T78 60T94 103T137 121Q165 121 187 96T210 8Q210 -27 201 -60T180 -117T154 -158T130 -185T117 -194Q113 -194 104 -185T95 -172Q95 -168 106 -156T131 -126T157 -76T173 -3V9L172 8Q170 7 167 6T161 3T152 1T140 0Q113 0 96 17Z"></path><path stroke-width="0" id="E13-MJMATHI-61" d="M33 157Q33 258 109 349T280 441Q331 441 370 392Q386 422 416 422Q429 422 439 414T449 394Q449 381 412 234T374 68Q374 43 381 35T402 26Q411 27 422 35Q443 55 463 131Q469 151 473 152Q475 153 483 153H487Q506 153 506 144Q506 138 501 117T481 63T449 13Q436 0 417 -8Q409 -10 393 -10Q359 -10 336 5T306 36L300 51Q299 52 296 50Q294 48 292 46Q233 -10 172 -10Q117 -10 75 30T33 157ZM351 328Q351 334 346 350T323 385T277 405Q242 405 210 374T160 293Q131 214 119 129Q119 126 119 118T118 106Q118 61 136 44T179 26Q217 26 254 59T298 110Q300 114 325 217T351 328Z"></path><path stroke-width="0" id="E13-MJMAIN-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E13-MJMAIN-28" x="0" y="0"></use><g transform="translate(389,0)"><use xlink:href="#E13-MJMATHI-73" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E13-MJMATHI-74" x="663" y="-213"></use></g><use xlink:href="#E13-MJMAIN-2C" x="1213" y="0"></use><g transform="translate(1657,0)"><use xlink:href="#E13-MJMATHI-61" x="0" y="0"></use><use transform="scale(0.707)" xlink:href="#E13-MJMATHI-74" x="748" y="-213"></use></g><use xlink:href="#E13-MJMAIN-29" x="2542" y="0"></use></g></svg></span><script type="math/tex">(s_t, a_t)</script></li><li><span>The reward is a numeric value. Can be negative (“punishment”).</span></li><li><span>Can be given throughout the learning episode, or only in the end.</span></li><li><strong><span>Goal</span></strong><span>: Maximize the total reward</span></li></ul><h5><a name="maximizing-total-reward" class="md-header-anchor"></a><span>Maximizing total reward</span></h5><p><span>Future rewards may be uncertain. We might care more about the rewards that  come sooner. Therefore, we discount future rewards.</span></p><p><span>A good strategy for an agent would be to always choose an action that </span><strong><span>maximizes the (discounted) future reward.</span></strong></p><h5><a name="action-selection" class="md-header-anchor"></a><span>Action Selection</span></h5><p><span>At each learning stage, the RL algorithm looks at the </span><strong><span>possible actions</span></strong><span> and calculates the </span><strong><span>expected average reward</span></strong><span>.</span></p><p><span>An action will be selected using:</span></p><ul><li><span>Greedy strategy: pure exploitation</span></li><li><span>Greedy strategy: exploitation with a little exploration</span></li><li><span>Soft-Max strategy</span></li></ul><h5><a name="policy-pi-and-value-v" class="md-header-anchor"></a><span>Policy </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.138ex" height="2.762ex" viewBox="0 -831.5 1351 1189" role="img" focusable="false" style="vertical-align: -0.83ex;"><defs><path stroke-width="0" id="E16-MJMAIN-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path stroke-width="0" id="E16-MJMATHI-3C0" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path><path stroke-width="0" id="E16-MJMAIN-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E16-MJMAIN-28" x="0" y="0"></use><use xlink:href="#E16-MJMATHI-3C0" x="389" y="0"></use><use xlink:href="#E16-MJMAIN-29" x="962" y="0"></use></g></svg></span><script type="math/tex">(\pi)</script><span> and Value </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.593ex" height="2.762ex" viewBox="0 -831.5 1547 1189" role="img" focusable="false" style="vertical-align: -0.83ex;"><defs><path stroke-width="0" id="E17-MJMAIN-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path stroke-width="0" id="E17-MJMATHI-56" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path><path stroke-width="0" id="E17-MJMAIN-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E17-MJMAIN-28" x="0" y="0"></use><use xlink:href="#E17-MJMATHI-56" x="389" y="0"></use><use xlink:href="#E17-MJMAIN-29" x="1158" y="0"></use></g></svg></span><script type="math/tex">(V)</script></h5><ul><li><span>The set of actions we took define our policy </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.138ex" height="2.762ex" viewBox="0 -831.5 1351 1189" role="img" focusable="false" style="vertical-align: -0.83ex;"><defs><path stroke-width="0" id="E16-MJMAIN-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path stroke-width="0" id="E16-MJMATHI-3C0" d="M132 -11Q98 -11 98 22V33L111 61Q186 219 220 334L228 358H196Q158 358 142 355T103 336Q92 329 81 318T62 297T53 285Q51 284 38 284Q19 284 19 294Q19 300 38 329T93 391T164 429Q171 431 389 431Q549 431 553 430Q573 423 573 402Q573 371 541 360Q535 358 472 358H408L405 341Q393 269 393 222Q393 170 402 129T421 65T431 37Q431 20 417 5T381 -10Q370 -10 363 -7T347 17T331 77Q330 86 330 121Q330 170 339 226T357 318T367 358H269L268 354Q268 351 249 275T206 114T175 17Q164 -11 132 -11Z"></path><path stroke-width="0" id="E16-MJMAIN-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E16-MJMAIN-28" x="0" y="0"></use><use xlink:href="#E16-MJMATHI-3C0" x="389" y="0"></use><use xlink:href="#E16-MJMAIN-29" x="962" y="0"></use></g></svg></span><script type="math/tex">(\pi)</script></li><li><span>The expected rewards we get in return, defines our value </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="3.593ex" height="2.762ex" viewBox="0 -831.5 1547 1189" role="img" focusable="false" style="vertical-align: -0.83ex;"><defs><path stroke-width="0" id="E17-MJMAIN-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path stroke-width="0" id="E17-MJMATHI-56" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path><path stroke-width="0" id="E17-MJMAIN-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E17-MJMAIN-28" x="0" y="0"></use><use xlink:href="#E17-MJMATHI-56" x="389" y="0"></use><use xlink:href="#E17-MJMAIN-29" x="1158" y="0"></use></g></svg></span><script type="math/tex">(V)</script></li></ul><h5><a name="markov-decision-process" class="md-header-anchor"></a><span>Markov Decision Process</span></h5><p><span>If we only need to know the </span><strong><span>current state</span></strong><span>, the problem has the </span><strong><em><span>Markov property</span></em></strong><span>.</span></p><h5><a name="value" class="md-header-anchor"></a><span>Value</span></h5><p><span>The expected future reward is known as the </span><strong><em><span>value.</span></em></strong></p><p><span>Two ways to compute the value:</span></p><ul><li><span>The value of a state, </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="4.682ex" height="2.762ex" viewBox="0 -831.5 2016 1189" role="img" focusable="false" style="vertical-align: -0.83ex;"><defs><path stroke-width="0" id="E18-MJMATHI-56" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path><path stroke-width="0" id="E18-MJMAIN-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path><path stroke-width="0" id="E18-MJMATHI-73" d="M131 289Q131 321 147 354T203 415T300 442Q362 442 390 415T419 355Q419 323 402 308T364 292Q351 292 340 300T328 326Q328 342 337 354T354 372T367 378Q368 378 368 379Q368 382 361 388T336 399T297 405Q249 405 227 379T204 326Q204 301 223 291T278 274T330 259Q396 230 396 163Q396 135 385 107T352 51T289 7T195 -10Q118 -10 86 19T53 87Q53 126 74 143T118 160Q133 160 146 151T160 120Q160 94 142 76T111 58Q109 57 108 57T107 55Q108 52 115 47T146 34T201 27Q237 27 263 38T301 66T318 97T323 122Q323 150 302 164T254 181T195 196T148 231Q131 256 131 289Z"></path><path stroke-width="0" id="E18-MJMAIN-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E18-MJMATHI-56" x="0" y="0"></use><use xlink:href="#E18-MJMAIN-28" x="769" y="0"></use><use xlink:href="#E18-MJMATHI-73" x="1158" y="0"></use><use xlink:href="#E18-MJMAIN-29" x="1627" y="0"></use></g></svg></span><script type="math/tex">V(s)</script><span>, averaged over all possible actions in that state </span><strong><em><span>(state-value function)</span></em></strong><span>.</span></li><li><span>The value of a state/action pair </span><strong><em><span>(action-value function)</span></em></strong></li><li><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.837ex" height="2.486ex" viewBox="0 -772.3 791 1070.5" role="img" focusable="false" style="vertical-align: -0.693ex;"><defs><path stroke-width="0" id="E19-MJMATHI-51" d="M399 -80Q399 -47 400 -30T402 -11V-7L387 -11Q341 -22 303 -22Q208 -22 138 35T51 201Q50 209 50 244Q50 346 98 438T227 601Q351 704 476 704Q514 704 524 703Q621 689 680 617T740 435Q740 255 592 107Q529 47 461 16L444 8V3Q444 2 449 -24T470 -66T516 -82Q551 -82 583 -60T625 -3Q631 11 638 11Q647 11 649 2Q649 -6 639 -34T611 -100T557 -165T481 -194Q399 -194 399 -87V-80ZM636 468Q636 523 621 564T580 625T530 655T477 665Q429 665 379 640Q277 591 215 464T153 216Q153 110 207 59Q231 38 236 38V46Q236 86 269 120T347 155Q372 155 390 144T417 114T429 82T435 55L448 64Q512 108 557 185T619 334T636 468ZM314 18Q362 18 404 39L403 49Q399 104 366 115Q354 117 347 117Q344 117 341 117T337 118Q317 118 296 98T274 52Q274 18 314 18Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E19-MJMATHI-51" x="0" y="0"></use></g></svg></span><script type="math/tex">Q</script><span> and </span><span class="MathJax_SVG" tabindex="-1" style="font-size: 100%; display: inline-block;"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.786ex" height="2.074ex" viewBox="0 -772.3 769 892.8" role="img" focusable="false" style="vertical-align: -0.28ex;"><defs><path stroke-width="0" id="E20-MJMATHI-56" d="M52 648Q52 670 65 683H76Q118 680 181 680Q299 680 320 683H330Q336 677 336 674T334 656Q329 641 325 637H304Q282 635 274 635Q245 630 242 620Q242 618 271 369T301 118L374 235Q447 352 520 471T595 594Q599 601 599 609Q599 633 555 637Q537 637 537 648Q537 649 539 661Q542 675 545 679T558 683Q560 683 570 683T604 682T668 681Q737 681 755 683H762Q769 676 769 672Q769 655 760 640Q757 637 743 637Q730 636 719 635T698 630T682 623T670 615T660 608T652 599T645 592L452 282Q272 -9 266 -16Q263 -18 259 -21L241 -22H234Q216 -22 216 -15Q213 -9 177 305Q139 623 138 626Q133 637 76 637H59Q52 642 52 648Z"></path></defs><g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)"><use xlink:href="#E20-MJMATHI-56" x="0" y="0"></use></g></svg></span><script type="math/tex">V</script><span> are initially unknown, and learned iteratively as we gain experience.</span></li></ul><h4><a name="the-q-learning-algorithm" class="md-header-anchor"></a><span>The Q-Learning Algorithm</span></h4><p><img src="C:\Users\Vi Dan Ba Mai\AppData\Roaming\Typora\typora-user-images\image-20200514010146209.png" referrerpolicy="no-referrer" alt="image-20200514010146209"></p><h4><a name="the-sarsa-algorithm" class="md-header-anchor"></a><span>The SARSA Algorithm</span></h4><p><img src="C:\Users\Vi Dan Ba Mai\AppData\Roaming\Typora\typora-user-images\image-20200514010633807.png" referrerpolicy="no-referrer" alt="image-20200514010633807"></p><h5><a name="on-policy-vs-off-policy-learning" class="md-header-anchor"></a><span>On-policy vs off-policy learning</span></h5><ul><li><p><span>Reward structure: </span></p><ul><li><span>Each move: -1</span></li><li><span>Move to cliff: -100</span></li></ul></li><li><p><span>Policy:</span></p><ul><li><span>90% chance of choosing best action (exploit). 10% chance of choosing random action (explore)</span></li></ul></li></ul><h5><a name="q-learning" class="md-header-anchor"></a><span>Q-learning</span></h5><ul><li><span>Always assumes optimal action -&gt; does not visit cliff often while learning. Therefore, does not learn that cliff is dangerous</span></li><li><span>Resulting path is efficient, but risky</span></li></ul><h5><a name="sarsa" class="md-header-anchor"></a><span>SARSA</span></h5><ul><li><span>During learning, we more frequently end up outside the cliff (due to 10% chance of exploring in our policy)</span></li><li><span>That info propagates to all states, generating a safer plan</span></li></ul><p><img src="C:\Users\Vi Dan Ba Mai\AppData\Roaming\Typora\typora-user-images\image-20200514011046592.png" referrerpolicy="no-referrer" alt="image-20200514011046592"></p><h3><a name="conclusion-from-weekly-exercise-12" class="md-header-anchor"></a><span>Conclusion from weekly exercise 12</span></h3><p><strong><span>Two core RL problems:</span></strong></p><ul><li><span>Policy evaluation</span></li><li><span>Policy improvement</span></li></ul><p><span>The problem with assessing how good the policy is.</span></p><p><span>For </span><strong><span>deterministic policies</span></strong><span> we can solve the problem simply by computing the </span><em><span>total discounted return</span></em><span> of a policy. There is an inherent limitation to deterministic policies, so a solution might be </span><strong><span>stochastic policies</span></strong><span>, and we compute their </span><em><span>return in expectation</span></em><span> relying on </span><em><span>Monte Carlo methods.</span></em></p><p><span>For policy improvement, the problem is learning optimal policies. A global value for a policy is not handy for editing the policy. Instead local values would be more useful. We can then compute </span><strong><span>state values</span></strong><span> and </span><strong><span>action values</span></strong><span> to assess a policy step by step, and use these values to choose improved actions. This iterative approach, based on evaluating the policy and improving the policy, is an instance of the more generic method </span><strong><span>Genralized Policy Iteration</span></strong><span>.</span></p><p><span>We can also consider a more advanced RL algorithm: Q-Learning.</span></p><p><span>Q-Learning allows us to leartn a optimal policy within an environment; this algorithm has two important features: (1) it learns in a continuous way, step-by-step, and not on complete episodes. (2) It learns an optimal policy while behaving according to a different, more exploration-prone policy.</span></p><p>&nbsp;</p></div>
</body>
</html>